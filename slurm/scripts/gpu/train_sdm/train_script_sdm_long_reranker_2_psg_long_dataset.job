#!/bin/bash
#SBATCH --nodes 1
#SBATCH -p gpu
#SBATCH --gres gpu:1
#SBATCH -t 00-10:00:00
#SBATCH --job-name s_2_l
#SBATCH --mem 256G
#SBATCH -o slurm/output/out/sdm_long_reranker_2_psg_long_dataset.out
#SBATCH -e slurm/output/err/sdm_long_reranker_2_psg_long_dataset.err
# Make conda available:
module purge
module load 2022
module load Miniconda3/4.12.0
module load CUDA/11.7.0
module load Java/11.0.2

export MAVEN_HOME=../maven/apache-maven-3.8.6/
export PATH=$MAVEN_HOME/bin:$PATH 

# Activate a conda environment:

source activate lsr
python -m lsr.train +experiment=reranker_qmlp_dmlm_msmarco_doc_2_psg_long resume_from_checkpoint=lsr42/qmlp_dmlm_msmarco_distil_kl_l1_0.0001 training_arguments.fp16=True training_arguments.per_device_train_batch_size=32 +training_arguments.learning_rate=0.001 training_arguments.max_steps=20001 +model.window_sizes=[1,2] +model.proximity=8 training_arguments.evaluation_strategy="steps" +training_arguments.eval_steps=20000 training_arguments.save_steps=20000 +training_arguments.save_total_limit=1 +training_arguments.metric_for_best_model="RR@10" +training_arguments.per_device_eval_batch_size=64